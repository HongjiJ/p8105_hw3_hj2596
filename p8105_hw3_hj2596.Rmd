---
title: "Homework 3 solutions"
author: "Hongji Jiang"
date: "2022-10-15"
output: github_document

---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Due date

Due: October 15 at 11:59pm. 

### Points

| Problem         | Points    |
|:--------------- |:--------- |
| Problem 0       | 20        |
| Problem 1       | --        |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |


### Problem 0

This solution focuses on a reproducible report containing code and text necessary for Problems 1-3, and is organized as an R Project. This was not prepared as a GitHub repo; examples for repository structure and git commits should be familiar from other elements of the course.

Throughout, we use appropriate text to describe our code and results, and use clear styling to ensure code is readable. 

### Problem 1

#### Read in the data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

#### Answer questions about the data

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

Import and clean the data. Create a weekday vs weekend variable 
### Problem 2
```{r}
#import the data
accel_df = read_csv( "./accel_data.csv") %>%
#clean names
janitor::clean_names() %>%
#Create a variable weekend where weekend is TRUE if day is weekend
#And FALSE if day is not weekend
mutate(weekend = ifelse(day %in% c("Saturday", "Sunday"),FALSE,TRUE)) %>%
pivot_longer(
  activity_1:activity_1440,
  names_to = "minute",
  names_prefix = "activity_",
  values_to = "activity_numbers"
  ) %>%
  mutate(minute = as.numeric(minute))
```

Aggregate across minutes to create a total activity variable for each day and create a table. The order is changed to Monday to Sunday. No apparent trend is observed. The sum of activity numbers is very low on Saturdays of the 4th and 5th week.
```{r}
accel_df %>%
        #change the order to monday to friday
        mutate(day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>%
        group_by(week, day) %>%
        summarize(activity_numbers_sum = sum(activity_numbers)) %>%
        pivot_wider(names_from = "day",values_from = "activity_numbers_sum") %>%
        knitr::kable()
```

```{r}
accel_df %>%
        ggplot(aes(x=minute,y=activity_numbers,color=day)) +
        geom_line(alpha = 0.5)
```
```{r}
library(p8105.datasets)
data("ny_noaa")
```

```{r}
summary(ny_noaa)
```
The dataset contains the id, date, prcp, snow, snwd, tmax and tmin. There are seven variables(columns) and 2595176 rows(observations). 
prcp has 145838 NA values.
snow has 381221 NA values.
snwd has 591786 NA values.
There are many NA values in the dataset and this may cause problems when we analyze the data.

```{r}
noaa_df = ny_noaa %>%
  janitor::clean_names() %>% 
  separate(col = date, into = c('year','month','day'), sep = "-" , convert = TRUE) %>%
  mutate (tmax = as.integer(tmax),
          tmin = as.integer(tmin),
          prcp = as.integer(prcp),
          year = as.integer(year),
          month = as.integer(month),
          day = as.integer(day)) %>% 
  mutate(tmax = tmax/10, tmin = tmin/10, prcp = prcp/10)

```


```{r}
sort(table(noaa_df$snow),decreasing=TRUE)[1]
```
For snowfall, the most common observed value is 0. Becase the variable snow refers to snowfall(mm), and it is most common to see the days with no snowfall at all. That is how we see the most common value is 0.

```{r}
noaa_df %>%
  mutate(month = month.name[month]) %>%
  filter(month %in% c("January","July")) %>% 
  group_by(id,year,month) %>%
  summarize(tmax_mean = mean(tmax),na.rm=TRUE) %>%
  ggplot(aes(x=year,y=tmax_mean, groups=id, color = tmax_mean))+
  geom_line(alpha = 0.5)+
  facet_grid(.~month)+
  theme(legend.position = "none") +
  labs(
  x = "Year",
  y = "Average Maximum Temperature (in Celsius)",
  title = "Average Maximum Temperature in January and July in Different Years")
```
We can observe that the average maximum temperature in January is much lower than the average maximum temperature in July, and this makes sense because this is the weather in New York and it is winter in January and summer in July. The temperatures in January mostly fall in the range between -10 and 10 C. And the temperatures in July mostly fall in the range between 20 and 35. Both the average max temperature in January and July over the years have some outliers with extreme high or low temperatures.

Because in the chunk below I am making a scatterplot with lots of data, thereâ€™s a limit to how much you can avoid overplotting using alpha levels. Using 'geom_hex' is a way to avoid that.
```{r}
tmax_vs_tmin = 
  noaa_df %>%
  drop_na(tmin, tmax) %>%
  drop_na() %>%
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  theme(legend.position = "none") +
  labs(
  x = "Minimum Temperature (Celsius)", 
  y = "Maximum Temperature (Celsius)",
  title = "Comparing Maximum with Minimum Temperature")
  

density_plot = 
  noaa_df %>%
  drop_na(snow) %>%
  mutate(year=as.factor(year)) %>%
  filter(snow>0 & snow<100) %>% 
  ggplot(aes(x = snow,y = year))+
  geom_density_ridges()+
  theme(legend.position = "none") +
  labs(
  x = "Snowfall(mm)",
  y = "Year",
  title = "Distribution of snowfall by year") 
tmax_vs_tmin+density_plot 
```
From the two-panel plot, we can see that the max temperature and min temperature could be related. As the min temperature gets higher, the max temperature also gets higher. Also we see that the temperature counts mostly falls in the range between -30 to 30.

